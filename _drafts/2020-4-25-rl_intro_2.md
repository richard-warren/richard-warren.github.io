---
title: 'reinforcement learning: intro (part 2)'
date: 2020-4-25
permalink: /blog/rl_intro_2/
tags:
  - reinforcement learning
  - machine learning
read_time: false
---


part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2 part 2

{% include toc %}
<br>

## temporal difference learning

### combining bootstrapping and sampling
Let's step back and consider the differences between dynamic programming and Monte Carlo. In Monte Carlo we estimate $v_\pi$ by **sampling** and averaging returns. In dynamic programming, we **bootstrap**, using the estimated value of subsequent states to figure out the value of the current state. **Temporal difference learning** is a model-free approach that combines sampling and bootstrapping. It is an important idea both in reinforcement learning [and neuroscience](https://deepmind.com/blog/article/Dopamine-and-temporal-difference-learning-A-fruitful-relationship-between-neuroscience-and-AI).

Recall that in Monte Carlo we compute returns from *full trajectories* of experience:

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \gamma^{T-1} R_{T}
$$

This means we have to wait until the experience is over to compute the return[^episodic] :thumbsdown:. In TD learning we *estimate* returns after observing only $R_{t+1}$, relying on the fact that returns can be expressed recursively:

[^episodic]: In this post we are only considering *episodic* tasks - those that have a beginning and an end (e.g. a game of chess). Many interesting tasks are *continuous*, which means waiting until the end of an experience to compute the return is not possible.

$$
\begin{aligned}
G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \gamma^{T-1} R_{T} \\
&= R_{t+1} + \gamma G_{t+1}
\end{aligned}
$$

Finally, in our *estimated returned* (which we will call a **target** for reasons that will soon become clear), we will replace $G_{t+1}$ with our estimate of the value of the next state, $V(S_{t+1})$. This is reasonable because $v_\pi(S_{t+1}) = \mathbb{E}[G_{t+1} \mid S_{t+1}]$.

$$ \color{blue}{(\text{target at time } t)} = \underbrace{R_{t+1}}_\text{sample!} + \gamma \underbrace{V(S_{t+1})}_\text{bootstrap!} $$


Okay! In Monte Carlo we estimated a state's value as the average return for that state. Similarly, we can compute the average *target* for each state by storing all targets for all states, then summing and dividing by the number of targets for each update. The following incremental update rule [is mathematically equivalent](http://incompleteideas.net/book/first/ebook/node19.html), but it only requires storing the number of times each state is visited, $N(s)$:

$$
V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)} [\color{blue}{R_t + \gamma V(S_{t+1})} - V(S_t)]
$$

This equation is telling us something. In brackets, the target (blue) is compared with the current value estimate. When the target is higher than the expected return (i.e. when we get more reward than we expect), we increase the value function for this state. We we get less than we expect, we decrease the value function. The difference between the target and the return is called the **TD error**, $\delta_t$. It tells us how 'surprised' we should be be the estimated return we just observed.

$$ \delta_t = \color{blue}{R_t + \gamma V(S_{t+1})} - V(S_t) $$

Let's make one last adjustment to our update rule. When we average estimated returns, we assign equal weight to returns collected at all times. This makes sense if the environment is **stationary**. But many environments *change over time*. Consider a chess opponent who learns your style of play over the course of the game. Her strategy (which is part of the environment) will change, and your value function should change accordingly. Similarly, if our policy is being updated over time, we will need to change our value function to keep up.

For these reasons it often makes sense to weight recent experiences more heavily when averaging. By replacing $\frac{1}{N(S_t)}$ with a learning rate $\alpha \in [0,1]$, we have an exponentially weighted average that 'forgets' old episodes over time:

$$
\begin{aligned}
V(S_t) &\leftarrow V(S_t) + \alpha [\color{blue}{R_t + \gamma V(S_{t+1})} - V(S_t)] \\
&= V(S_t) + \alpha \delta_t
\end{aligned}
$$

Now that we have a *prediction* algorithm, let's consider how we can use TD learning to do *control*. Recall that state-value functions $v_\pi(s)$ aren't terribly useful if we don't know the environment dynamics $p(s',r \mid s,a)$ (because knowing state values doesn't help if we don't know which actions will lead to which states). Therefore, as we did with Monte Carlo control, we will learn action-value functions $q_\pi(s,a)$, and select actions that maximize $q_\pi(s,a)$.

We will use backup-diagrams to illustrate three methods for learning $q_\pi(s,a)$. Recall that black dots are actions and white circles are states.


### SARSA
![](/images/sutton_barto_notes/backup_sarsa.PNG){: .align-right}

We can learn action-value functions using an update rule analogous to the state-value update described above. Given a **S** tate, we select an **A** ction, observe the **R** eward and subsequent **S** tate, and then select the next **A** ction. The target is the reward we get immediately, plus the discounted value of the next state-action pair, $\color{blue}{R_{t+1}  + \gamma Q(S_{t+1}, A_{t+1})}$. Using this target, the update rule for **SARSA** is:

$$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [ \color{blue}{R_{t+1}  + \gamma Q(S_{t+1}, A_{t+1})} - Q(S_t, A_t)] $$

Note that this is an on-policy method because were are always selecting actions *according to the current policy*.


### Q-learning
![](/images/sutton_barto_notes/backup_qlearning.PNG){: .align-right}

SARSA learns the action-value function associated with a given policy. The policy can then be improved by acting greedily with respect $Q(s,a)$. **Q-learning** is an off-policy algorithm that directly approximates $q_{* }(s,a)$, which is the action-value function associated with the optimal policy $\pi_{* }$. To do this, we take *the best* action when bootstrapping, as opposed to the action chosen by our current policy:

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [ \color{blue}{R_{t+1}  + \gamma \max_a Q(S_{t+1}, a)} - Q(S_t, A_t)]
$$

The 'arc' in the backup diagram shown on the right symbolizes the 'max' operation. Note that this is an *off-policy* algorithm because actions aren't selected according to the current policy.


### expected SARSA
![](/images/sutton_barto_notes/backup_expectedsarsa.PNG){: .align-right}

In SARSA, $A_{t+1}$ is a random variable that is selected according to our policy. For stochastic policies (such as $epsilon$-greedy policies), this randomness introduces variance that can be mitigated by taking the expectation of subsequent actions. This is called **expected SARSA**.

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [ \color{blue}{R_{t+1}  + \gamma \sum_a \pi(a|S_{t+1}) Q(S_{t+1}, a)} - Q(S_t, A_t)]
$$

Here we don't select the second action, but instead take the expectation over actions we could take. This expecation is with respect to the action distribution defined by the current policy. Expected SARSA is therefore considered on-policy, although this is perhaps [up for debate](https://people.cs.umass.edu/~rrmenon/papers/sarsa18.pdf) :thinking:.


## $\lambda$ return

### splitting the sampling-bootstrap difference
todo: something about striking balance between monte carlo and DP


### n-step bootstrapping
In Monte Carlo we sample entire trajectories. This method is low bias but has high variance due to sampling error. In TD(0) we sample one step and then bootstrap on our value function. This method has bias but much lower variance.

Although these approaches may appear qualitatively different, they are in fact the extremes of a continuum. In general we can take $n$ steps and then bootstrap. The number of steps determines the bias-variance trade-off (bigger $n \rightarrow$ less bias, more variance):

![image](reinforcement_learning/nstep.png){width=".7\linewidth"}

The $n$ step return and its associated update rule (for state-value functions) are: $$\begin{gathered}
G_{t:t+n} = R_{t+1} + \gamma  R_{t+2} + \gamma^2  R_{t+3} + \dots + \gamma^{n-1}  R_{t+n} + V(S_{t+n}) \\ \\
V(S_t) \leftarrow V(S_t) + \alpha [G_{t:t+n} - V(S_t)]\end{gathered}$$

Intuitively, taking larger $n$ allows us to see further back when assigning credit/blame to states and actions that led to reward. For example, upon reaching a reward state in TD(0) we only updated the value of the states/actions that caused reward, but the actions *that led to* the actions that caused reward are not updated. With $n=10$, however, we can assign credit to several states in the sequence that resulted in reward:

![image](reinforcement_learning/nstep_paths.png){width=".8\linewidth"}


### $\lambda$ return
$n$-step returns ($G_{t:t+n}$) allow us to control the extent to which we rely on sampling (big $n$) vs. bootstrapping (small $n$). Rather than picking a single $n$, we can take take an exponentially weighted average of $n$-step returns for all $n$: $$G_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1}G_{t:t+n}$$ $(1-\lambda)$ ensure the weights sum to 1. Note that for $\lambda=0$ this reduces to a one step return, whereas for $\lambda=1$ it is a monte carlo return. The weighting scheme is visualized as follows. Notice that all returns which reach the terminal state are collectively given the rest of the weight:

![image](reinforcement_learning/lambda_return.JPG){width=".6\linewidth"}

For continuous settings we can't compute $n$-step returns for arbitrarily large $n$. We can therefore approximate $G_t^\lambda$ by taking the first $k$ returns. By setting all $G_{t:t+k+\delta} =G_{t:t+k}$ for $\delta \in \mathbb{N}$ we give all the residual weight to $G_{t:t+k}$:

$$\begin{aligned}
G_{t:t+k}^\lambda
&= (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1}G_{t:t+n} \\
&= (1-\lambda) \sum_{n=1}^{k-1} \lambda^{n-1}G_{t:t+n} + (1-\lambda) \sum_{n=k}^{\infty} \lambda^{n-1}G_{t:t+n} \\
&= (1-\lambda) \sum_{n=1}^{k-1} \lambda^{n-1}G_{t:t+n} + (1-\lambda) \sum_{n=k}^{\infty} \lambda^{n-1}G_{t:t+k} & \scriptstyle{\text{we are setting $G_{t:t+k+\delta} =G_{t:t+k} $ for $\delta \in \mathbb{N}$}}\\
&= (1-\lambda) \sum_{n=1}^{k-1} \lambda^{n-1}G_{t:t+n} + \lambda^{k-1}(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t:t+k} & \scriptstyle{\text{re-index and factor out $\lambda^{k-1}$}} \\
&= (1-\lambda) \sum_{n=1}^{k-1} \lambda^{n-1}G_{t:t+n} + \underbrace{\lambda^{k-1}G_{t:t+k}}_\text{residual weight} & \scriptstyle{\text{because $\sum_{n=1}^{\infty} \lambda^{n-1} = \frac{1}{1-\lambda}$}}\end{aligned}$$


### offline $\lambda$ return algorithm
The *offline $\lambda$ return algorithm* performs weight updates after each episode according to the $\lambda$ return at each time point using semi-gradient ascent as follows: $$\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \left[G_t^\lambda - \hat{v}(S_t, \mathbf{w}_t) \right] \nabla_\mathbf{w} \hat{v}(S_t, \mathbf{w}_t)$$


### online $\lambda$ return algorithm
We can perform truncated $\lambda$ return updates online, but this requires a trick. If we are interested in $5$-step $\lambda$ returns, but we are only at $t=2$, we start by computing the $2$ -step return. Then when $t=3$, we go back and update our previous weight updates using the most recently available returns, and so on.


## forward vs. backward views
Value functions are expectations over returns, which are a function of *future* rewards. Every time we encounter a state, we update it's value in the direction of immediate rewards, and to a lesser extent distant rewards. We continue moving state by state, associating rewards with states based on their temporal proximity.

If a reward occurs soon *after* a state, we associate the reward with the state. By the same token, if a state occur soon *before* a reward, we should make the same association. This *backward view* is appealing because it opens the door to efficient, online algorithms for value function updates.

For the backward view to work, we need to keep track of how recently we visited each state. Just as rewards are discounted exponentially as time passes, our recency metric will fade exponentially. This recency metric is called as *eligibility trace*, $\mathbf{z}$. If we have a one-hot vector representation of the state, $\mathbf{x}$, then when we visit state $s$ we have $x_s \leftarrow x_s+1$ and $x_{i} \leftarrow \gamma \lambda x_i$. For each $x_i$ this causes decay patterns like this:

![image](reinforcement_learning/trace_decay.png){width=".7\linewidth"}


### TD($\lambda$)
TD($\lambda$) *approximates* the offline $\lambda$-return using an efficient backward view algorithm. We first need to generalize eligibility traces to function approximation. Recall that the general formulation for weight updates is: $$\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \delta_t \nabla_\mathbf{w} \hat{v}(S_t, \mathbf{w}_t)$$ $\delta_t$ is the difference between the target and our current value estimate. This tells us how much we should be surprised by what is happening *now*. The associated update rule for the eligibility trace is: $$\mathbf{z}_{t} = \gamma \lambda \mathbf{z}_{t-1} + \nabla \hat{v}(S_t, \mathbf{w}_{t})$$ This more general eligibility trace no longer strictly reflects the recency of states. Rather, it reflects how elegible each element in $\mathbf{w}$ is for tweaking when something good or bad happens. It is like a smeared out version of the gradient. The new weight update is: $$\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t$$ At each time point, $\nabla \hat{v}(S_t, \mathbf{w}_{t})$ tells us how we would change each element in $\mathbf{w}$ to increase the value estimate of $S_t$. When things go better than expected, $\delta_t$ will be high. We then nudge $\mathbf{w}$ in the direction that increases the value of recent states, because $\mathbf{z}$ contains the gradients for past states exponentially weighted by their recency.

Notice that in the special case where $\mathbf{x}$ is a one-hot representation of the state and value function is linear, e.g. $\hat{v}(\mathbf{x}, \mathbf{w}) = \mathbf{w}^\top \mathbf{x}$, then $\nabla_\mathbf{w} \hat{v} = \mathbf{x}$ and the gradient based update rule is equivalent to the initial updated rule where 1 is added to only to the active state, and all other states decay.

We can construct SARSA($\lambda$) similarly by replacing the state-value function $v$ with the action-value function $q$.


### true TD($\lambda$) and dutch traces
TD($\lambda$) only approximates the offline $\lambda$-return. However, by tweaking the eligibility trace and weight update formulas, we can achieve a backward view algorithm that is equivalent to the online $\lambda$-return. The new *dutch trace* and associated weight update are: $$\begin{gathered}
\mathbf{z}_{t} = \gamma \lambda \mathbf{z}_{t-1} + (1 - \alpha \gamma \lambda\mathbf{z}_{t-1}^\top \mathbf{x}_{t})\mathbf{x}_{t} \\
\mathbf{w}_{t+1} = \mathbf{w}_{t} + \alpha \delta_t \mathbf{z}_{t} + \alpha (\mathbf{w}_{t}^\top\mathbf{x}_{t} - \mathbf{w}_{t-1}^\top\mathbf{x}_{t}) (\mathbf{z}_{t} - \mathbf{x}_{t})\\\end{gathered}$$ A proof for the equivalence of the forward and backward views using dutch traces is provided in the text for the case of monte carlo control without discounting (section 12.6).


## the big picture

### breadth vs. depth
The methods we've applied so far vary in breadth and depth. Methods that bootstrap (DP and TD) are shallow in that they only step a little into the future before bootstrapping. Monte Carlo is deep in that it samples until the end of trajectories. Furthermore, methods that take expectations over actions are wide, such as DP, whereas both TD and Monte Carlo can be thought of as narrow because they rely on sampling rather than averaging over actions.

Methods exist in between these extremes and can be thought of as living in a space characterized by two axes. These axes represent the extent to which a method relies on sampling vs. expectations (narrow vs. wide), and bootstrapping vs. sampling (shallow vs. deep):

![image](reinforcement_learning/rl_bigpicture.PNG){width=".6\linewidth"}


### value function updates
The value function updates we've considered differ in whether they 1) update the state or action value function, 2) approximate the optimal or an arbitrary policy, and 3) rely on sample or expected updates. The different combinations of these three binary dimensions yield the following family of value function updates:

![image](reinforcement_learning/value_updates.PNG){width=".5\linewidth"}


## up next
Teaser for [part 3](/blog/rl_intro_3).
