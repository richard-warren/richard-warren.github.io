---
title: 'reinforcement learning (4/4): policy gradient'
date: 2020-5-14
permalink: /blog/rl_intro_4/
tags:
  - reinforcement learning
  - machine learning
read_time: false
---


The techniques discussed in parts 1 and 2 are effective in settings where state and action spaces are small and discrete. Here I summarize [Sutton and Barto's](http://incompleteideas.net/book/the-book-2nd.html) chapters on value function approximation and policy iteration. These tools will allow us to tackle more interesting problems consisting of large or continuous action and state spaces. The math is a bit heavier :nerd_face:, but so is the payoff.


{% include toc %}
<br>

# policy gradient
Up to this point we have been learning value functions, which can be used to guide policy. Policy gradient methods directly learn the parameters for the policy. To do this we gradually tweak our policy parameters $\theta$ in the direction that increases the amount of reward we expect under our policy. We therefore need to take the gradient of some performance metric $J(\theta)$ with respect to $\theta$ and update according to: $\theta_{t+1} = \theta_t + \nabla J(\theta)$.

## policy gradient theorem
There's a difficultly with this approach. The amount of reward we expect depends on the policy, which we can differentiate, but also on the distribution of states, which depends on potentially complex interactions between the policy and the environment. The policy gradient theorem solves this problem.

Consider the episodic case, in which performance can be defined as the expected return in the start state, $v(s_0)$. The policy gradient theorem gives the following derivative, where $\mu(s)$ is the stationary distribution of states under $\pi$:

$$\begin{aligned}
\nabla J(\theta) &= \nabla v(s_0) \\
&= \nabla \sum_s \mu(s) \sum_a \pi(s \mid a) q(s,a) \\
&\propto \sum_s \mu(s) \sum_a  q(s,a) \nabla \pi(s \mid a) \\\end{aligned}$$ The magic happens in the third line. The policy gradient theorem says we can take the gradient with respect to the policy without worrying about the gradient of the state distribution.

It turns out the policy gradient theorem also holds in continuing problems if we define $J(\theta)$ to be the average *rate* of reward: $$J(\theta) = \sum_s \mu(s) \sum_a \pi (a \mid s) \sum_{s',r} p(s',r \mid s, a) r$$

## policy gradient theorem proof
In episodic settings we want to take the gradient of $J(\theta) = v(s_0)$. Let's start by taking the gradient for any $s$. First we'll establish a recursive relationship between the value of a state and the subsequent state. For simplicity we will not use discounting: $$\begin{aligned}
\nabla v(s)
&= \nabla \sum_a \pi(a \mid s) q(s,a) \\
&= \sum_a \left[ \nabla \pi(a \mid s) q(s,a) + \pi(a \mid s) \nabla q(s,a) \right] & \scriptstyle{\text{product rule}} \\
&= \sum_a \left[ \nabla \pi(a \mid s) q(s,a) + \pi(a \mid s) \nabla \sum_{s',r}p(s',r\mid s,a)[r + v(s')] \right] & \scriptstyle{\text{express $q$ as an expectation over states}} \\
&= \sum_a \left[ \nabla \pi(a \mid s) q(s,a) + \pi(a \mid s) \sum_{s'}p(s'\mid s,a)\nabla v(s') \right] \\
&= \phi(s) + \sum_a \pi(a \mid s) \sum_{s}p(s'\mid s,a)\nabla v(s') & \scriptstyle{\text{set $\phi(s) = \sum_a \nabla \pi(a \mid s) q(s,a)$}}\end{aligned}$$ Get ready for some funny notation. $\rho(s \rightarrow x, k)$ will denote the probability of moving from state $s$ to state $x$ in $k$ steps under the current policy. Note that $\rho(s_0 \rightarrow s_2, 2) = \sum_{s_1} \rho(s_0 \rightarrow s_1, 1) \rho(s_1 \rightarrow s_2, 1)$. To get from $s_0$ to $s_2$, we must add up all the probabilities involving all possible intermediary $s_1$.

$$\begin{aligned}
\nabla v(s)
&= \phi(s) + \sum_a \pi(a \mid s) \sum_{s'}p(s'\mid s,a)\nabla v(s') \\
&= \phi(s) + \sum_{s'} \sum_a \pi(a \mid s) p(s'\mid s,a)\nabla v(s') \\
&= \phi(s) + \sum_{s'} \rho(s \rightarrow s', 1) \nabla v(s') \\
&= \phi(s) + \sum_{s'}\rho(s \rightarrow s', 1) \left[\phi(s') + \sum_{s''} \rho(s' \rightarrow s'', 1) \nabla v(s'') \right] & \scriptstyle{\text{recurse!}} \\
&= \phi(s) + \sum_{s'} \rho(s \rightarrow s', 1) \phi(s') + \sum_{s''} \rho(s \rightarrow s'', 2) \nabla v(s'') & \scriptstyle\text{$\rho(s \rightarrow s'', 2) = \sum_{s'} \rho(s \rightarrow s', 1) \rho(s' \rightarrow s'', 1)$}\\
&= \sum_{x \in \mathcal{S}} \sum_{k=0}^\infty \rho(s \rightarrow x, k) \phi(x) \\
&= \sum_{x \in \mathcal{S}} \sum_{k=0}^\infty \rho(s \rightarrow x, k) q(s,a) \nabla\pi(s \mid a)\end{aligned}$$ Now let's consider what happens with $s_0$: $$\begin{aligned}
\nabla v(s_0)
&= \sum_{s} \sum_{k=0}^\infty \rho(s_0 \rightarrow s, k) \phi(x) \\
&= \sum_{s} \eta(s) \phi(s) & \scriptstyle{\text{$\eta(s)$ is the average time in state $s$ in across episodes}} \\
&= \sum_{s'}\eta(s') \sum_{s} \frac{\eta(s)}{\sum_{s'}\eta(s')} \phi(x) & \scriptstyle{\text{normalize to probability distribution}} \\
&= \sum_{s'} \eta(s') \sum_{s} \mu(s) \phi(x) \\
&\propto \sum_{s} \mu(s) \sum_a q(s,a) \nabla \pi(s \mid a) \\\end{aligned}$$

## REINFORCE
Let's put the policy gradient theorem to use. Although we don't know the true distribution of states $\mu(s)$, we can continuously sample states under the current policy to approximate the distribution without bias. This monte carlo approach relies of the fact that:0 $$\begin{aligned}
\nabla J(\theta)
&\propto \sum_{s} \mu(s) \sum_a q(s,a) \nabla \pi(s \mid a) \\
&= \mathbb{E}_\pi \left[ \sum_a q(S_t,a) \nabla \pi(s \mid a) \right] & \scriptstyle{\text{replacing $s$ with the sample $S_t$}}\end{aligned}$$ Similarly, rather than performing updates by considering all actions, we can perform updates by sampling one action at a time because: $$\begin{aligned}
\nabla J(\theta)
&= \mathbb{E}_\pi \left[ \sum_a q(S_t,a) \nabla \pi(a \mid s) \right] \\
&= \mathbb{E}_\pi \left[ \sum_a \pi(a \mid s) q(S_t,a) \frac{\nabla \pi(a \mid s)}{\pi(a \mid s)} \right] \\
&= \mathbb{E}_\pi \left[q(S_t, A_t) \frac{\nabla \pi(A_t \mid s)}{\pi(A_t \mid s)} \right] && \scriptstyle{\text{replacing $a$ with the sample $A_t$}} \\
&= \mathbb{E}_\pi \left[ G_t \frac{\nabla \pi(s \mid a)}{\pi(s \mid a)} \right] && \scriptstyle{\text{because $\mathbb{E}[G_t \mid S_t, A_t] = q(S_t,A_t)$}} \\\end{aligned}$$ This leads to the stochastic update rule $\theta_{t+1} = \theta_t + \alpha G_t \frac{\nabla \pi(A_t \mid S_t)}{\pi(A_t \mid S_t)}$, or equivalently $\theta_{t+1} = \theta_t + \alpha G_t \nabla \ln \pi(A_t \mid S_t)$ (because $\nabla \ln x = \frac{\nabla x}{x}$).

We can reduce the variance of our updates without introducing bias by subtracting a baseline from $G_t$. The baseline can be anything that doesn't depend on $a$, such as a current estimate of the state value: $$\theta_{t+1} = \theta_t + \alpha (G_t - \hat{v}(S_t)) \nabla \ln \pi(A_t \mid S_t)$$

The following algorithm uses monte carlo to learn both the weights for the policy $\theta$ and the weights of the value function $\mathbf{w}$:

![image](/images/blog/rl_imgs//reinforce_baseline.JPG){: .align-center}

## actor-critic
REINFORCE has no bias because it relies on sample returns, but it can suffer from high variance. We can reduce the variance (while introducing bias) by constructing bootstrapped targets, e.g. by replacing $G_t$ with a one-step TD target:

$$\theta_{t+1} = \theta + \alpha \left[R_{t+1} + \gamma \hat{v}(S_{t+1}) - \hat{v}(S_t) \right] \nabla \ln \pi (A_t \mid S_t)$$

Here the *critic* learns the value function and the *actor* learns to update the policy to increase future reward.

## policy parameterizations
Policy gradient methods present a natural way of dealing with large or continuous action spaces. Rather than learning probability mass functions over many different actions, we can directly learn the parameters of probability distributions, for example the mean and standard deviation of gaussians in action space.


# learning your model
Thus far we have discussed model-free algorithms and algorithms that require a model of the world, $p(s',r \mid a,a)$, such as dynamic programming. When utilizing models we can distinguish between *real* and *simulated* experience. Real experience results from interactions with the world (e.g. observing the consequences of taking action $a$ in state $s$), whereas simulated experience results from querying the model (e.g. the model predicts what would happen if the agent were to take action $a$ in state $s$). There is a related distinction between *planning* and *learning*. Planning uses a model (simulated experience) to find better actions, whereas learning relies upon real experience with the environment.

We can also consider algorithms that *learn* a model of the world through experience. For such algorithms, the utility of real experience is two-fold: it can be used to update the value function (*direct reinforcement learning*) in addition to improving the model (*model learning*):

![image](/images/blog/rl_imgs//planning_learning.PNG){: .align-center}

## DynaQ
DynaQ is a simple algorithm that utilizes both planning and learning. The agent takes actions (b,c), updates the value function based on the observed reward via Q-learning (direct RL; d), updates the model of the world based on that experience (using a simple lookup table in this example; e), then uses the model to generate simulated experience that further refines the value function (f):

![image](/images/blog/rl_imgs//dynaq.PNG){: .align-center}
