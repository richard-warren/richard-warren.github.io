---
title: 'reinforcement learning: intro (part 2)'
date: 2020-5-14
permalink: /blog/rl_intro_2/
tags:
  - reinforcement learning
  - machine learning
read_time: false
---


The techniques discussed in part 1 are powerful in settings where state and action spaces are small and discrete. Here I summarize [Sutton and Barto's](http://incompleteideas.net/book/the-book-2nd.html) chapters on value function approximation and policy iteration. These tools will allow us to tackle more interesting problems consisting of large or continuous action and state spaces. The math is a bit heavier :nerd_face:, but so is the payoff.


{% include toc %}
<br>

## value function approximation
The tabular methods we have discussed thus far are limited when the action and/or state spaces are very large, and impossible when they are continuous. Value function approximation solves both problems by using a function to approximate the relationship between a representation of the state and the value of that state. A value function parameterized by weights $\mathbf{w}$ is denoted $\hat{v}(s, \mathbf{w})$.Note that there is a lot of flexibility in how we represent the state. In general, we will represent each state $s$ by a vector $\mathbf{x}(s)$.

### stochastic gradient descent
We can learn the value function (i.e. its parameters $\mathbf{w}$) via gradient descent. This requires defining a loss functions and taking its gradient $\nabla$ with respect to $\mathbf{w}$. We will compute the mean squared error between the estimated and true value function, averaged across states with weights $u(s)$ ($\mu(s)$ is typically the *stationary distribution* of states under the current policy): $$\overline{VE}(\mathbf{w}) = \sum_{s \in \mathcal{S}} u(s) \left[v_\pi(s) - \hat{v}_\pi(s, \mathbf{w})\right]^2$$ We can make our value estimate more accurate by nudging $\mathbf{w}$ in the direction of the gradient. Because the states should be distributed according to $u(s)$, the expectation for these updates should be the true update. Recall that the gradient is a vector describing how each variable in a scalar valued function affects the output: $\nabla_\mathbf{w} f(\mathbf{w}) = \left( \frac{\partial f(\mathbf{w})}{\partial w_1}, \frac{\partial f(\mathbf{w})}{\partial w_2}, \dots, \frac{\partial f(\mathbf{w})}{\partial w_d} \right)^\top$. We will oftentimes omit the subscript in $\nabla_\mathbf{w}$.

Our stochastic gradient ascent algorithm will nudge $\mathbf{w}$ in the direction of the gradient for each $S_t$ we observe: $$\begin{aligned}
\mathbf{w_{t+1}} &= \mathbf{w_{t}} + \alpha \frac{1}{2} \nabla \left[v_\pi(s) - \hat{v}_\pi(S_t, \mathbf{w})\right]^2 \\
&= \mathbf{w_{t}} + \alpha \left[{v_\pi(S_{t})} - \hat{v}_\pi(S_t, \mathbf{w})\right] \nabla \hat{v}_\pi(S_t, \mathbf{w}) & \scriptstyle{\text{chain rule}}\end{aligned}$$ So far so good, but if we knew $v_\pi(s)$ we wouldn't need to be reading this book! Instead we use a *target* to approximate $v_\pi(s)$. If the target is a real return, $G_t$, the updates will be unbiased because the value function is the expectation over returns. A gradient monte carlo algorithm therefore updates the weights according to: $$\mathbf{w_{t+1}} = \mathbf{w_{t}} + \alpha \left[{\color{blue}G_t} - \hat{v}_\pi(S_t, \mathbf{w})\right] \nabla \hat{v}_\pi(S_t, \mathbf{w})$$ We could alternatively use a TD(0) target : $$\mathbf{w_{t+1}} = \mathbf{w_{t}} + \alpha \left[{\color{blue}R_t + \gamma \hat{v}_\pi(S_{t+1}, \mathbf{w})} - \hat{v}_\pi(S_t, \mathbf{w})\right] \nabla \hat{v}_\pi(S_t, \mathbf{w})$$ There's a subtlety here: when we took the gradient above, the original target $v_\pi(s)$ did not depend on $\mathbf{w}$. But our esimated target, $\hat{v}(s, \mathbf{w})$, depends on $\mathbf{w}$. Using a bootstrapped target containing our estimated value function invalidates the math. This approach is therefore called *semi-gradient*. It still works in practice, but at the expense of some theoretical guarantees.

### state representations
Imagine a task in which the true dimensionality of the state space is $\mathbb{R}^2$. In mountain car, for example, the state is simply the position and velocity of the car. In this case the true value function (for the optimal policy) can be visualized as a surface in $\mathbb{R}^2$, and it is quite complex:

![image](reinforcement_learning/mountaincar.PNG){width=".4\linewidth"}

![image](reinforcement_learning/mountaincar_value.PNG){width=".4\linewidth"}

Now consider a linear value function that takes a weighted sum of the state representation, $\hat{v}(s, \mathbf{w}) = \mathbf{w}^\top \mathbf{x}(s)$. If $\mathbf{x}(s)$ is just a vector containing the position and velocity, then we could only create planar value functions that intersect the origin. We could fix this by making the value function more complex, e.g. a neural network. An alternative solution is to build a state representation $\mathbf{x}(s)$ that allows more complex value functions to be constructed.

*Polynomials*. In polynomial bases, each element $x_i(s)$ is a product of some combination the original elements raised to some power. For example, if the position and velocity in mountain car are $(s_1, s_2)$, then we could have $\mathbf{x}(s) = (1, s_1, s_2, s_1s_2, s_1^2s_2, s_1s_2^2,s_1^2s_2^2)$. Now we can create more complex value function surfaces in $\mathbb{R}^2$.

*Fourier basis.* Periodic (well-behaved) functions of arbitrary complexity can be represented as a sum of trigonometric functions. Using this logic, we can create state representations where each element is a trigonometric function of our original data with a characteristic frequency in each dimension: $x_i(s) = cos(\pi \mathbf{s}^\top c_i)$. The elements of each $c_i$ control the frequency of each dimensions, as follows:

![image](reinforcement_learning/fourier_basis.JPG){width=".4\linewidth"}

*Tiling.* What if we just slice up our state space into a grid, with $\mathbf{x}(s)$ becoming a one-hot vector representation of our grid location? With a linear value function, this actually turns the problem back into a lookup table, with $\mathbf{w}$ now storing the estimated values of each state. This shows that function approximation is actually a generalization of the tabular case. We can be more clever though. If we have several grids, each offset from one another, then every point in the space will activate multiple, partially overlapping grid points.

![image](reinforcement_learning/tiling.JPG){width=".8\linewidth"}

Overlapping grids allow generalization to occur. We can also encourage generalization in particular dimensions/directions by creating longer rectangles or even full stripes that extend in the direction that generalization should occur.

### function approximators
Anything differentiable can be used for $\hat{v}$, such as an artificial neural network or linear regression.

There are also non-parametric approaches to function approximation. For example, a nearest neighbors approach could take the mean of the $k$ visited states that are closest to the current state. Or we could take a mean across all visited states weighted by their closeness. Kernel methods use a function $k(s,s')$ that assigns a weight for each $s'$ when estimating the value of $s$: $\hat{v}(s,\mathbf{w}) = \sum_{s' \in \mathcal{D}} k(s,s') g(s')$. Here $g(s')$ is the target (e.g. return for monte carlo, bootstraped return for TD(0)) for the previously visited state $s'$.

### control with function approximation
To use function approximation for control rather than prediction, we can learn an action-value function and follow a policy that acts greedily with respect to it. Following the pattern above, we can update our weights (for example by following a SARSA target) as: $$\mathbf{w_{t+1}} = \mathbf{w_{t}} + \alpha \left[{\color{blue}R_t + \gamma \hat{q}_\pi(S_{t+1}, A_{t+1}, \mathbf{w})} - \hat{q}_\pi(S_t, A_t, \mathbf{w})\right] \nabla \hat{v}_\pi(S_{t}, \mathbf{w})$$

## policy gradient
Up to this point we have been learning value functions, which can be used to guide policy. Policy gradient methods directly learn the parameters for the policy. To do this we gradually tweak our policy parameters $\theta$ in the direction that increases the amount of reward we expect under our policy. We therefore need to take the gradient of some performance metric $J(\theta)$ with respect to $\theta$ and update according to: $\theta_{t+1} = \theta_t + \nabla J(\theta)$.

### policy gradient theorem
There's a difficultly with this approach. The amount of reward we expect depends on the policy, which we can differentiate, but also on the distribution of states, which depends on potentially complex interactions between the policy and the environment. The policy gradient theorem solves this problem.

Consider the episodic case, in which performance can be defined as the expected return in the start state, $v(s_0)$. The policy gradient theorem gives the following derivative, where $\mu(s)$ is the stationary distribution of states under $\pi$:

$$\begin{aligned}
\nabla J(\theta) &= \nabla v(s_0) \\
&= \nabla \sum_s \mu(s) \sum_a \pi(s \mid a) q(s,a) \\
&\propto \sum_s \mu(s) \sum_a  q(s,a) \nabla \pi(s \mid a) \\\end{aligned}$$ The magic happens in the third line. The policy gradient theorem says we can take the gradient with respect to the policy without worrying about the gradient of the state distribution.

It turns out the policy gradient theorem also holds in continuing problems if we define $J(\theta)$ to be the average *rate* of reward: $$J(\theta) = \sum_s \mu(s) \sum_a \pi (a \mid s) \sum_{s',r} p(s',r \mid s, a) r$$

### policy gradient theorem proof
In episodic settings we want to take the gradient of $J(\theta) = v(s_0)$. Let's start by taking the gradient for any $s$. First we'll establish a recursive relationship between the value of a state and the subsequent state. For simplicity we will not use discounting: $$\begin{aligned}
\nabla v(s)
&= \nabla \sum_a \pi(a \mid s) q(s,a) \\
&= \sum_a \left[ \nabla \pi(a \mid s) q(s,a) + \pi(a \mid s) \nabla q(s,a) \right] & \scriptstyle{\text{product rule}} \\
&= \sum_a \left[ \nabla \pi(a \mid s) q(s,a) + \pi(a \mid s) \nabla \sum_{s',r}p(s',r\mid s,a)[r + v(s')] \right] & \scriptstyle{\text{express $q$ as an expectation over states}} \\
&= \sum_a \left[ \nabla \pi(a \mid s) q(s,a) + \pi(a \mid s) \sum_{s'}p(s'\mid s,a)\nabla v(s') \right] \\
&= \phi(s) + \sum_a \pi(a \mid s) \sum_{s}p(s'\mid s,a)\nabla v(s') & \scriptstyle{\text{set $\phi(s) = \sum_a \nabla \pi(a \mid s) q(s,a)$}}\end{aligned}$$ Get ready for some funny notation. $\rho(s \rightarrow x, k)$ will denote the probability of moving from state $s$ to state $x$ in $k$ steps under the current policy. Note that $\rho(s_0 \rightarrow s_2, 2) = \sum_{s_1} \rho(s_0 \rightarrow s_1, 1) \rho(s_1 \rightarrow s_2, 1)$. To get from $s_0$ to $s_2$, we must add up all the probabilities involving all possible intermediary $s_1$.

$$\begin{aligned}
\nabla v(s)
&= \phi(s) + \sum_a \pi(a \mid s) \sum_{s'}p(s'\mid s,a)\nabla v(s') \\
&= \phi(s) + \sum_{s'} \sum_a \pi(a \mid s) p(s'\mid s,a)\nabla v(s') \\
&= \phi(s) + \sum_{s'} \rho(s \rightarrow s', 1) \nabla v(s') \\
&= \phi(s) + \sum_{s'}\rho(s \rightarrow s', 1) \left[\phi(s') + \sum_{s''} \rho(s' \rightarrow s'', 1) \nabla v(s'') \right] & \scriptstyle{\text{recurse!}} \\
&= \phi(s) + \sum_{s'} \rho(s \rightarrow s', 1) \phi(s') + \sum_{s''} \rho(s \rightarrow s'', 2) \nabla v(s'') & \scriptstyle\text{$\rho(s \rightarrow s'', 2) = \sum_{s'} \rho(s \rightarrow s', 1) \rho(s' \rightarrow s'', 1)$}\\
&= \sum_{x \in \mathcal{S}} \sum_{k=0}^\infty \rho(s \rightarrow x, k) \phi(x) \\
&= \sum_{x \in \mathcal{S}} \sum_{k=0}^\infty \rho(s \rightarrow x, k) q(s,a) \nabla\pi(s \mid a)\end{aligned}$$ Now let's consider what happens with $s_0$: $$\begin{aligned}
\nabla v(s_0)
&= \sum_{s} \sum_{k=0}^\infty \rho(s_0 \rightarrow s, k) \phi(x) \\
&= \sum_{s} \eta(s) \phi(s) & \scriptstyle{\text{$\eta(s)$ is the average time in state $s$ in across episodes}} \\
&= \sum_{s'}\eta(s') \sum_{s} \frac{\eta(s)}{\sum_{s'}\eta(s')} \phi(x) & \scriptstyle{\text{normalize to probability distribution}} \\
&= \sum_{s'} \eta(s') \sum_{s} \mu(s) \phi(x) \\
&\propto \sum_{s} \mu(s) \sum_a q(s,a) \nabla \pi(s \mid a) \\\end{aligned}$$

### REINFORCE
Let's put the policy gradient theorem to use. Although we don't know the true distribution of states $\mu(s)$, we can continuously sample states under the current policy to approximate the distribution without bias. This monte carlo approach relies of the fact that:0 $$\begin{aligned}
\nabla J(\theta)
&\propto \sum_{s} \mu(s) \sum_a q(s,a) \nabla \pi(s \mid a) \\
&= \mathbb{E}_\pi \left[ \sum_a q(S_t,a) \nabla \pi(s \mid a) \right] & \scriptstyle{\text{replacing $s$ with the sample $S_t$}}\end{aligned}$$ Similarly, rather than performing updates by considering all actions, we can perform updates by sampling one action at a time because: $$\begin{aligned}
\nabla J(\theta)
&= \mathbb{E}_\pi \left[ \sum_a q(S_t,a) \nabla \pi(a \mid s) \right] \\
&= \mathbb{E}_\pi \left[ \sum_a \pi(a \mid s) q(S_t,a) \frac{\nabla \pi(a \mid s)}{\pi(a \mid s)} \right] \\
&= \mathbb{E}_\pi \left[q(S_t, A_t) \frac{\nabla \pi(A_t \mid s)}{\pi(A_t \mid s)} \right] && \scriptstyle{\text{replacing $a$ with the sample $A_t$}} \\
&= \mathbb{E}_\pi \left[ G_t \frac{\nabla \pi(s \mid a)}{\pi(s \mid a)} \right] && \scriptstyle{\text{because $\mathbb{E}[G_t \mid S_t, A_t] = q(S_t,A_t)$}} \\\end{aligned}$$ This leads to the stochastic update rule $\theta_{t+1} = \theta_t + \alpha G_t \frac{\nabla \pi(A_t \mid S_t)}{\pi(A_t \mid S_t)}$, or equivalently $\theta_{t+1} = \theta_t + \alpha G_t \nabla \ln \pi(A_t \mid S_t)$ (because $\nabla \ln x = \frac{\nabla x}{x}$).

We can reduce the variance of our updates without introducing bias by subtracting a baseline from $G_t$. The baseline can be anything that doesn't depend on $a$, such as a current estimate of the state value: $$\theta_{t+1} = \theta_t + \alpha (G_t - \hat{v}(S_t)) \nabla \ln \pi(A_t \mid S_t)$$

The following algorithm uses monte carlo to learn both the weights for the policy $\theta$ and the weights of the value function $\mathbf{w}$:

![image](reinforcement_learning/reinforce_baseline.jpg){width=".9\linewidth"}

### actor-critic
REINFORCE has no bias because it relies on sample returns, but it can suffer from high variance. We can reduce the variance (while introducing bias) by constructing bootstrapped targets, e.g. by replacing $G_t$ with a one-step TD target:

$$\theta_{t+1} = \theta + \alpha \left[R_{t+1} + \gamma \hat{v}(S_{t+1}) - \hat{v}(S_t) \right] \nabla \ln \pi (A_t \mid S_t)$$

Here the *critic* learns the value function and the *actor* learns to update the policy to increase future reward.

### policy parameterizations
Policy gradient methods present a natural way of dealing with large or continuous action spaces. Rather than learning probability mass functions over many different actions, we can directly learn the parameters of probability distributions, for example the mean and standard deviation of gaussians in action space.


## learning your model
Thus far we have discussed model-free algorithms and algorithms that require a model of the world, $p(s',r \mid a,a)$, such as dynamic programming. When utilizing models we can distinguish between *real* and *simulated* experience. Real experience results from interactions with the world (e.g. observing the consequences of taking action $a$ in state $s$), whereas simulated experience results from querying the model (e.g. the model predicts what would happen if the agent were to take action $a$ in state $s$). There is a related distinction between *planning* and *learning*. Planning uses a model (simulated experience) to find better actions, whereas learning relies upon real experience with the environment.

We can also consider algorithms that *learn* a model of the world through experience. For such algorithms, the utility of real experience is two-fold: it can be used to update the value function (*direct reinforcement learning*) in addition to improving the model (*model learning*):

![image](reinforcement_learning/planning_learning.PNG){width=".4\linewidth"}

### DynaQ
DynaQ is a simple algorithm that utilizes both planning and learning. The agent takes actions (b,c), updates the value function based on the observed reward via Q-learning (direct RL; d), updates the model of the world based on that experience (using a simple lookup table in this example; e), then uses the model to generate simulated experience that further refines the value function (f):

![image](reinforcement_learning/dynaq.PNG){width="1\linewidth"}
